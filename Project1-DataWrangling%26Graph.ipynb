{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "matplotlib.rcParams['savefig.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The New York Social Graph\n",
    "\n",
    "[New York Social Diary](http://www.newyorksocialdiary.com/) provides a\n",
    "fascinating lens onto New York's socially well-to-do.  The data forms a natural social graph for New York's social elite.  Take a look at this page of a recent [run-of-the-mill holiday party](http://www.newyorksocialdiary.com/party-pictures/2014/holiday-dinners-and-doers).\n",
    "\n",
    "Besides the brand-name celebrities, you will notice the photos have carefully annotated captions labeling those that appear in the photos.  We can think of this as implicitly implying a social graph: there is a connection between two individuals if they appear in a picture together.\n",
    "\n",
    "For this project, we will assemble the social graph from photo captions for parties dated December 1, 2014, and before.  Using this graph, we can make guesses at the most popular socialites, the most influential people, and the most tightly coupled pairs.\n",
    "\n",
    "We will attack the project in three phases:\n",
    "1. Get a list of all the photo pages to be analyzed.\n",
    "2. Parse all of the captions on a sample page.\n",
    "3. Parse all of the captions on all pages, and assemble the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase One\n",
    "\n",
    "The first step is to crawl the data.  We want photos from parties on or before December 1st, 2014.  Go to the [Party Pictures Archive](http://www.newyorksocialdiary.com/party-pictures) to see a list of (party) pages.  We want to get the url for each party page, along with its date.\n",
    "\n",
    "We use Python Requests to download the HTML pages, and BeautifulSoup to process the HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we try to get the number of party pages for the 95 months (that is, month-year pair) in the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import requests\n",
    "import dill\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "#nltk.download()\n",
    "import nltk\n",
    "url = \"http://www.newyorksocialdiary.com/party-pictures\"\n",
    "response = requests.get(url)\n",
    "from requests_futures.sessions import FuturesSession\n",
    "\n",
    "#this is a function to get the link and the date\n",
    "def get_link_date(el):\n",
    "    #get the link\n",
    "    the_link = 'http://www.newyorksocialdiary.com' + el.a['href']   \n",
    "    #get the date   \n",
    "    #first child span\n",
    "    first_span = el.find('span', attrs={'class': 'views-field-created'})\n",
    "    #seond child spand that is actually the date of the party\n",
    "    second_span = first_span.find('span', attrs = {'class' : 'field-content'}).text.encode('utf-8')\n",
    "    #this following party_time retunrs the dates as strings\n",
    "    #party_date = datetime.strptime(second_span, '%A, %B %d, %Y').strftime('%Y, %m, %d')\n",
    "    #following returns the dates as date times\n",
    "    party_date = datetime.strptime(second_span, '%A, %B %d, %Y')\n",
    "    return [the_link, party_date]    \n",
    "    #date_list = [date.month, date.day, date.year]\n",
    " \n",
    "\n",
    "#running the following we can get the links and the dates in the first page\n",
    "def get_links(r):\n",
    "    #is the next line just for debugging?\n",
    "    r.text[:1000] + \"...\"\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "    l = soup.find_all('div', attrs={'class':'views-row'})\n",
    "    links = []\n",
    "    for link in l:\n",
    "        links.append(get_link_date(link))\n",
    "    return links\n",
    "\n",
    "#filter the dates\n",
    "def filter_by_date(links, cutoff=datetime(2014, 12, 1)):\n",
    "    before2014 = []\n",
    "    for l in links:\n",
    "        if l[1] <= cutoff:\n",
    "            before2014.append(l)\n",
    "    return before2014\n",
    "\n",
    "\n",
    "page1 = get_links(response)\n",
    "page1_before2014 = filter_by_date(page1 , cutoff=datetime(2014, 12, 1))\n",
    "assert len(filter_by_date(get_links(response))) == 0\n",
    "\n",
    "#all the links in all the pages that are related to before 2014\n",
    "link_list = []\n",
    "session = FuturesSession(max_workers=5)\n",
    "urls = [\"http://www.newyorksocialdiary.com/party-pictures?page=\" + str(i) for i in range(29)]\n",
    "futures = [session.get(url) for url in urls]\n",
    "for future in futures:\n",
    "    link_list.extend(get_links(future.result()))\n",
    "    \n",
    "link_list = filter_by_date(link_list, cutoff=datetime(2014, 12, 1))\n",
    "\n",
    "dill.dump(link_list, open('nysd-links.pkd', 'w'))\n",
    "#link_list is saved in the file so the next time you want the link_list\n",
    "#you can just run the following\n",
    "link_list = dill.load(open('nysd-links.pkd', 'r'))\n",
    "\n",
    "\n",
    "date_of_parties = [l[1] for l in link_list]\n",
    "month_of_parties = [d.strftime('%b-%Y') for d in date_of_parties]\n",
    "\n",
    "link_list_months = []\n",
    "\n",
    "for i in range(len(link_list)):\n",
    "    link_list_months.append([link_list[i][0], month_of_parties[i]])\n",
    "\n",
    "#histogram\n",
    "from collections import Counter\n",
    "def histogram():   \n",
    "     return list(Counter(month_of_parties).items())\n",
    "    \n",
    "histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase Two\n",
    "\n",
    "In this phase, we we concentrate on getting the names out of captions for a given page.It means that we need to parse the names out of the caption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to get the captions according to the URL\n",
    "def get_captions(url):\n",
    "    response = requests.get(url)\n",
    "    response.text[:1000] + \"...\"\n",
    "    soup = BeautifulSoup(response.text, \"lxml\")\n",
    "    caps = soup.find_all('div', attrs={'class':'photocaption'})\n",
    "    list_of_captions = []\n",
    "    for tag in caps:\n",
    "        list_of_captions.append(tag.text.strip().encode('utf-8'))\n",
    "    return list_of_captions\n",
    "\n",
    "#get the data for the specified URL\n",
    "url1 = \"http://www.newyorksocialdiary.com/party-pictures/2015/celebrating-the-neighborhood\"\n",
    "raw = get_captions(url1)\n",
    "#dump the content in a file so that you do not have to access and get the captions from the page every time\n",
    "dill.dump(raw, open('raw.pkd', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read the data from the file\n",
    "names = dill.load(open('raw.pkd', 'r'))\n",
    "#first split each caption\n",
    "import re\n",
    "for i in range(len(names)):\n",
    "    names[i]= re.sub(r'\\, ', ' & ', names[i]).split('&')\n",
    "\n",
    "#if there are just two names in a caption that are seprerate with an and, split\n",
    "for i in range(len(names)):\n",
    "    if len(names[i]) == 1:\n",
    "        names[i] = names[i][0].split(' and ')\n",
    "#get rid of the and before the last name in each caption       \n",
    "for i in range(len(names)):\n",
    "    names[i][-1] = re.sub(r'^ and ','', names[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#in the names, each caption is in the form a list, in order to be able to use nltk which works with strings, let's make \n",
    "#a string from each caption\n",
    "n = []\n",
    "for caption in names:\n",
    "    n.extend(caption)\n",
    "import os\n",
    "os.environ[\"STANFORD_MODELS\"] = \"/home/vagrant/ner/stanford-ner-2016-10-31\"\n",
    "import nltk.tag.stanford as st\n",
    "tagger = st.StanfordNERTagger(\"/home/vagrant/ner/stanford-ner-2016-10-31/classifiers/english.all.3class.distsim.crf.ser.gz\",\"/home/vagrant/ner/stanford-ner-2016-10-31/stanford-ner.jar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk_results = []\n",
    "for i in range(len(n)):\n",
    "    nltk_results.append(tagger.tag(n[i].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_page1= [[word for (word, tag) in name if tag == u'PERSON' ] for name in nltk_results]\n",
    "names_page1_formatted = [' '.join(name).encode('utf-8') for name in  names_page1]\n",
    "names_page1_formatted = [name for name in names_page1_formatted if name!='' ]\n",
    "#output is to get the unique names\n",
    "output = []\n",
    "for x in names_page1_formatted:\n",
    "    if x not in output:\n",
    "        output.append(x)\n",
    "print output\n",
    "# Scraping all of the pages could take 10 minutes or so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Phase Three\n",
    "\n",
    "For the remaining analysis, we think of the problem in terms of a network or a graph. Any time a pair of people appear in a photo together, that is considered a link. What we have described is more appropriately called an (undirected) multigraph with no self-loops but this has an obvious analog in terms of an undirected weighted graph. In this problem, we will analyze the social graph of the new york social elite. \n",
    "\n",
    "after completing phase one and two, we have ended up with over 100,000 captions and more than 110,000 names, connected in about 200,000 pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest question to ask is \"who is the most popular\"? The easiest way to answer this question is to look at how many connections everyone has. The following piece of code returns the top 100 people and their degree. We should remember that if an edge of the graph has weight 2, it counts for 2 in the degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq  # Heaps are efficient structures for tracking the largest\n",
    "              # elements in a collection.  Use introspection to find the\n",
    "              # function you need.\n",
    "import networkx as nx\n",
    "all_names = dill.load(open('all_names1.pkd', 'r'))\n",
    "G = nx.MultiGraph()\n",
    "from itertools import combinations\n",
    "def degree():\n",
    "    for page in all_names:\n",
    "        for caption in page:\n",
    "            connections = list(combinations(caption,2))\n",
    "            G.add_edges_from(list(combinations(caption,2)))\n",
    "    A = sorted(G.degree().items(), key = lambda x: -x[1])[:100]\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A similar way to determine popularity is to look at their pagerank. Pagerank is used for web ranking and was originally patented by Google and is essentially the stationary distribution of a markov chain implied by the social graph.\n",
    "We can use 0.85 as the damping parameter so that there is a 15% chance of jumping to another vertex at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools  # itertools.combinations may be useful\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "all_names = dill.load(open('all_names1.pkd', 'r'))\n",
    "\n",
    "def pagerank():\n",
    "    G = nx.MultiGraph()\n",
    "    for page in all_names:\n",
    "        for caption in page:\n",
    "            connections = list(combinations(caption,2))\n",
    "            G.add_edges_from(list(combinations(caption,2)))\n",
    "        \n",
    "    H = nx.Graph()\n",
    "    for u,v,w in G.edges(data=True):\n",
    "        if H.has_edge(u,v):\n",
    "            H[u][v]['weight'] += 1\n",
    "        else:\n",
    "            H.add_edge(u,v,weight=1)\n",
    "    ranks = nx.pagerank(H)\n",
    "    A = sorted(ranks.items(), key = lambda x: -x[1])[:100]\n",
    "    perfect_rank = []   \n",
    "    for i in range(100):\n",
    "        perfect_rank.append(A[i])\n",
    "    return perfect_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting question is who tend to co-occur with each other. Following piece of code gives uf the 100 edges with the highest weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G = nx.MultiGraph()\n",
    "for page in all_names:\n",
    "    for caption in page:\n",
    "        connections = list(combinations(caption,2))\n",
    "        G.add_edges_from(list(combinations(caption,2)))\n",
    "\n",
    "H = nx.Graph()\n",
    "for u,v,w in G.edges(data=True):\n",
    "    if H.has_edge(u,v):\n",
    "        H[u][v]['weight'] += 1\n",
    "    else:\n",
    "        H.add_edge(u,v,weight=1)\n",
    "ranks = nx.pagerank(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_friends():   \n",
    "    connections =  sorted(H.edges(data=True), key=lambda(source,target,data): -data['weight'])[:100]\n",
    "    l = []\n",
    "    for (u, v, w) in connections:\n",
    "         l.append(((u,v),w['weight']))\n",
    "    return l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
